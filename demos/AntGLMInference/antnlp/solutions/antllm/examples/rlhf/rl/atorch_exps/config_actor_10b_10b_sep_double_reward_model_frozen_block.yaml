model:
  actor_critic_ref:
      model_path:  /mnt1/xuantai.hxd/glm-10b-2k-stf-v9/glm-10b-2k-sft-v9
      model_cls:  solutions.antllm.antllm.models.glm.modeling_glm_ppo.AutoModelForGLMSeparate
      model_params: 
        num_layers_unfrozen: 2
        pretrained_critic_model_path: /mnt1/xuantai.hxd/glm-10b-2k-stf-v9/glm-10b-2k-sft-v9
      train_strategy: ./configs/ds_config_trlx_bf16.json
      inference_strategy: torch_native
      optimizer: 
        name: torch.optim.adamw
        kwargs: 
          lr: 1.0e-6
          betas: 
            - 0.9
            - 0.95
          eps: 1.0e-8
          weight_decay: 0.01
  reward_model:
      model_path: /mnt1/xuantai.hxd/glm-10b-2k-stf-v9/glm-10b-2k-sft-v9
      model_cls: solutions.antllm.antllm.models.glm.modeling_glm_rm.RewardModel
      train_strategy: ./benchmarks/glm_rlhf/shared_weights/strategy.py
      model_params: 
        use_position_id: false
        num_head: 1
        use_mean_value: false 
        use_normalized_reward: false
  cost_model:
      model_path: /mnt1/xuantai.hxd/glm-10b-2k-stf-v9/glm-10b-2k-sft-v9
      model_cls: solutions.antllm.antllm.models.glm.modeling_glm_rm.RewardModel
      train_strategy: ./benchmarks/glm_rlhf/shared_weights/strategy.py
      model_params: 
        use_position_id: false
        num_head: 1
        use_mean_value: false 
        use_normalized_reward: false
train:
  seq_length: 2000
  batch_size: 4
  eval_interval: 10
  checkpoint_interval: 100
  epoch: 10000
  num_rollouts: 32
  checkpoint_dir: /mnt1/xuantai.hxd/glm-10b-2k-stf-v9/glm-10b-2k-sft-v9
  gradient_accumulation_steps: 2
  max_grad_norm: 1
  scheduler:
    name: cosine_warmup
    kwargs:
      num_warmup_steps: 1000
      num_training_steps: 100000
generation:
    batch_size: 4
    gen_kwargs:
      max_new_tokens: 1000
      top_k: 0
      top_p: 1.0
      do_sample: false
    gen_experience_kwargs:
      max_new_tokens: 1000
      do_sample: false
      temperature: 1.0
      top_k: 50
      top_p: 0.95
tokenizer:
    tokenizer_path: /mnt1/xuantai.hxd/glm-10b-2k-stf-v9/glm-10b-2k-sft-v9
    params:
      truncation_side: left
      padding_side: right
method:
  PPOConfig:
      ppo_epoch: 1
      init_kl_coef: 0.2
      gamma: 1
      lam: 0.95
      cliprange: 0.2
      cliprange_value: 0.2
      vf_coef: 1.0
      cliprange_reward: 50
      clip_ratio: true
      ent_coef: 0
      horizon: 10000
      scale_reward: ignored
      ref_mean: null
      ref_std: null
